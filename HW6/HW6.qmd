---
title: "STATS 500 HW6"
author: "Minxuan Chen"
date: last-modified
format:
  html:
    toc: true
    css: styles.css
    fig-align: "center"
    fig-width: 6
    fig-height: 5
    embed-resources: true
    format-links: false
    execute:
      warning: true
      freeze: auto
  pdf:
    papersize: a4paper
    pdf-engine: pdflatex
    toc: false
    number-sections: false
    colorlinks: true
    fig-width: 6
    fig-asp: 0.8
    urlcolor: blue
    fontsize: 12pt
    geometry:
      - top=30mm
      - left=15mm
      - right=15mm
      - bottom=30mm
      - heightrounded
code-fold: show
code-overflow: scroll
code-line-numbers: true
---

```{=latex}
% Trigger ToC creation in LaTeX
\tableofcontents
\thispagestyle{empty}
\vspace{20pt}
Github repo: \url{https://github.com/PKUniiiiice/STATS_500}

\newpage

\setcounter{page}{1}
```

## Problem 1
### (a)
```{r p1a}
library(faraway)
library(quantreg)
library(MASS)
data("stackloss")

# least squares
lsq <- lm(stack.loss ~ ., data=stackloss)
summary(lsq)
```
Least squares works well and we observe a $R^2$ of 0.9136 and a adjusted $R^2$ of 0.8983. And the predictors `Air.Flow`, `Water.Temp` and the intercept are significant.

```{r p1aa}
#least absolute deviations
glad <- quantreg::rq(stack.loss ~ ., data=stackloss)
summary(glad)
```
There is some change in the coefficients. While the confidence intervals indicate significance for all predictors, we note that the upper bound of `Acid.Conc` is -0.02891, a value close to zero. This suggests that `Acid.Conc` may only be weakly significant. Additionally, although the estimate for `Water.Temp` has reduced to 0.57, which is half of the previous estimation, its confidence interval is relatively wide. The upper bound is 1.4109, encompassing the estimate from the least squares model.

```{r p1aaa}
#Huber method
ghuber <- MASS::rlm(stack.loss ~ ., data=stackloss)
summary(ghuber)
```
Again, there is some change in the coefficients. The confidence intervals suggest that `Acid.Conc` is not significant. And the standard error becomes smaller (2.441).

```{r p1aaaa}
#least trimmed squares
glts <- MASS::ltsreg(stack.loss ~ ., data=stackloss, nsamp="exact")
round(glts$coefficients, 4)
```
Comparing the results to the least squares model, we observe significant changes in the estimation of `Water.Temp`. In fact, its value falls outside the confidence interval of the least squares estimation. To obtain standard errors for the LTS regression coefficients, we employed a bootstrap method.
```{r p1aaaaa}
bcoef <- matrix(0,1000,4)
for(i in 1:1000){
  newy <- glts$fitted.values + glts$residuals[sample(21, rep=T)]
  bcoef[i,] <- MASS::ltsreg(stack.x, newy, nsamp="best")$coef
}
apply(bcoef,2,function(x) quantile(x,c(0.025,0.975)))
```
From the bootstrap, both `Water.Temp` and `Acid.Conc` are not significant.

Now, we use diagnostic methods to detect outliers or influential points. We consider the least squares model.

```{r, fig.height=6}
#outlier
plot(abs(rstudent(lsq)),ylim=c(0, 3.5))
abline(h=2, col='red')
text(x=c(4,21), y=abs(rstudent(lsq))[c(4,21)],
     labels=c(4,21), pos=3, col="red", cex=0.8)
car::outlierTest(lsq)
```
Based on this plot, it appears that there are no outliers among the data points. Furthermore, the test results confirm this observation, as the Bonferroni p-value exceeds 0.05, indicating the absence of outliers.

```{r}
#leverage and influential
par(mfrow=c(1,2))
plot(lsq, which=5)
abline(v=2*length(lsq$coefficients)/nrow(stackloss), col='red')
hatvalues(lsq)>2*length(lsq$coefficients)/nrow(stackloss)
halfnorm(cooks.distance(lsq),3,ylab="Cook’s distances")
par(mfrow=c(1,1))
```
There is only one point with high leverage, however, from the Cook's distance line, this point is not influential.

From the half-norm plot, case No.21 is likely to be influential.

From above, we remove case No.4 and No.21 and then use least squares.
```{r}
lsq.rm <- lm(stack.loss ~ ., data=stackloss, subset=-c(4,21))
summary(lsq.rm)
```
Comparing the results to the original least squares method, we observe obvious changes in the coefficients. Additionally, the $R^2$ value has increased, indicating a better fit.

### (b)
```{r p1b}
MASS::boxcox(lsq, plotit=T, lambda=seq(-1, 1.5, by=0.1))
```
The confidence interval of $\lambda$ is approximately $(-0.2, 0.7)$. Notably, $\lambda=1$ falls outside of this range. Therefore, it becomes necessary to consider some transformation of the `stack.loss` variable. For the sake of convenience and interpretation, we can explore two options: setting $\lambda=0$, which results in $\log(\texttt{stack.loss})$, or choosing $\lambda=0.5$, which leads to $\sqrt{\texttt{stack.loss}}$. The likelihood values for these two choices are close"

### (c)
We use the square root transformation.
```{r p1c, fig.height=8}
lsq.sqrt <- lm(sqrt(stack.loss) ~ ., data=stackloss)
summary(lsq.sqrt)
par(mfrow=c(2,2))
plot(lsq.sqrt)
par(mfrow=c(1,1))
```
From the residuals vs fitted values and QQ plots, there is no obvious issues regarding  linearity and normality. The scale-location plot also does not reveal any significant violations of homoscedasticity. Additionally, the residuals vs. Leverage plot indicates no clear influential points.

In general, after applying the Box-Cox transformation, the results of the least squares regression seem to align with the standard assumptions.

## Problem 2
### (a)
```{r p2a}
data("aatemp")
m1 <- lm(temp~year, data=aatemp)
summary(m1)
par(mfrow=c(1,2))
plot(m1, which=c(1,2))
par(mfrow=c(1,1))
```
Note that the estimation of the year variable is statistically significant, suggesting the presence of a non-constant linear trend.

In the QQ plot, there are no apparent issues. However, in the residuals vs. fitted plot, a slight horizontal "S" curve is discernible in the residuals, as indicated by the red lowess line. This observation implies structural problems within the linear model concerning the year variable. Therefore, it may be necessary to move to a nonlinear model.

### (b)
```{r p2b}
library(nlme)
m2.corr <- nlme::gls(temp~year,
                     correlation=corAR1(form=~year), data=aatemp)
summary(m2.corr)
intervals(m2.corr, which="var-cov")
```
The correlation is 0.2304, and the confidence interval does not contain 0. Therefore, we conclude that the correlation is statistically significant.

Regarding the linear trend, the estimated coefficient for the 'year' variable remains statistically significant at level of $\alpha=0.05$.' However, it's worth noting that its p-value is not particularly small, and the estimate is close to zero. As a result, we consider the linear trend in this case to be less significant compared to the one in (a).

### (c)
```{r}
#we start with poly(year, 10) and check whether the highest polynomial term is significant to decide whether to stop
m.try <- NULL
summ <- NULL
for (i in 10:1){
  summ <- summary(m.try <- lm(temp ~ poly(year, i), data=aatemp))
  deg.max <- summ$coefficients[i+1, "Pr(>|t|)"]
  if (deg.max<0.05) break
}
summ
```
Therefore, the highest degree polynomial to fit the model is 5.

The specific final model is a polynomial model of degree 5. And the regression equation is (ref: [click](https://stackoverflow.com/questions/26728289/extracting-orthogonal-polynomial-coefficients-from-rs-poly-function))
$$
\begin{aligned}
\text{temp} = &\beta_0F_0(\text{year}) + \beta_1F_1(\text{year})  +\beta_2F_2(\text{year}) + \\&\beta_3F_3(\text{year}) +\beta_4F_4(\text{year}) + \beta_5F_5(\text{year})
\end{aligned}
$$
in which, $\beta_i$ is as follows,
```{r}
beta <- coef(m.try)
print(sapply(1:length(beta), function(i) {
  paste("beta_", i-1, ": ", beta[i], sep = "")
}))
```
And $F_i(\text{year})$ is defined recursively by
$$
\begin{aligned}
&F_0(x) = 1 / \sqrt{n_2} \\
&F_1(x) = (x-a_1) / \sqrt{n_3}\\
&F_i(x) = \frac{{(x - a_i) \cdot \sqrt{n_{i+1}} \cdot F_{i-1}(x) - \frac{{n_{i+1}}}{{\sqrt{n_i}}} \cdot F_{i-2}(x)}}{{\sqrt{n_{i+2}}}},\quad i\geq2
\end{aligned}
$$
in which $a_i$ and $n_i$ are No.i element of the following vector
```{r}
ai <- attributes(z <- poly(aatemp$year, 5))$coefs$alpha
ni <- attributes(z)$coefs$norm2
F.i <- function(x, i){
  if(i==0){
    return (1/sqrt(ni[2]))
  }
  else if(i==1){
    return ((x-ai[1])/sqrt(ni[3]))
  }
  else{
    return ((((x-ai[i])*sqrt(ni[i+1]))*F.i(x, i-1) -
    ni[i+1]/sqrt(ni[i])*F.i(x, i-2))/sqrt(ni[i+2]))
  }
}
# ai
ai
# ni
ni
```

```{r}
#plot the fitted model on top of the data
plot(aatemp$year, aatemp$temp)
lines(aatemp$year, m.try$fitted.values, col="red")
```


For the polynomial model, $R^2$ is 0.1952 and adjusted $R^2$ is 0.1583. This result is better than the simple linear model. The plot also shows the model fits the data well. 

### (d)
Results of polynomial model
```{r p2d}
newx <- data.frame(year=2020)
new.temp <- predict(m.try, newdata=newx); new.temp
#confidence interval
predict(m.try, newdata = newx, interval = "confidence", level = 0.95)
#predictive interval
predict(m.try, newdata = newx, interval = "prediction", level = 0.95)
```
Results of simple linear model
```{r p2dd}
new.temp <- predict(m1, newdata=newx); new.temp
#confidence interval
predict(m1, newdata = newx, interval = "confidence", level = 0.95)
#predictive interval
predict(m1, newdata = newx, interval = "prediction", level = 0.95)
```

For this extrapolation, the polynomial model provides a higher temperature prediction for the year 2020 and a significantly wider confidence/prediction interval. This is primarily attributed to the inherent instability of polynomial methods for extrapolation (Runge's phenomenon). In reality, according to NOAA records, the annual mean temperature is approx 50°F. Therefore, the temperature prediction offered by the polynomial model is unreliable in this context. In general, both models raise doubts when extrapolating to such a distant future. However, the linear model outperforms the polynomial model significantly.

### (e)
```{r p2e}
plot(aatemp$year, aatemp$temp, main="Temp in Ann Arbor, MI")
abline(v=1930, col="red")
lhs <- function(x) ifelse(x<=1930, mean(aatemp$temp[1:49]), 0)
rhs <- function(x) ifelse(x>1930, x-1930, 0)
m2 <- lm(temp~lhs(year)+rhs(year), data=aatemp)
x <- aatemp$year
py <- m2$coef[1] + m2$coef[2]*lhs(x) + m2$coef[3]*rhs(x)
lines(x, py, lty=2)
summary(m2)
```

While the $R^2$ of this model is 0.1954, significantly higher than that of the simple linear model, we observe that the coefficient of `rhs(year)` is not statistically significant. We cannot assert the presence of a linear trend after 1930. Therefore, the assertion doesn't appears to be reasonable.

### (f)
```{r p2f}
library(splines)
knots <- c(1854, 1854, 1854, seq(1854, 2000, length.out=4),
          2000, 2000, 2000)
byear <-  splineDesign(knots, aatemp$year)
gs <-  lm(aatemp$temp ~ byear-1)
df.plot <- data.frame(cbind(aatemp$year, aatemp$temp,
                              gs$fitted.values,
                              m.try$fitted.values,
                              m2.corr$fitted,
                              m1$fitted.values,
                              py))
colnames(df.plot) <- c("year", "temp", "Spline",
                       "Polynomial","AR(1)",
                       "Simple", "Broken Stick")
library(ggplot2)
# Create a ggplot object
gg <- ggplot(data = df.plot, aes(x = year)) +
  geom_point(aes(y = temp), color = "skyblue") +
  geom_line(aes(y = Spline, color = "Spline")) +
  geom_line(aes(y = Polynomial, color = "Polynomial")) +
  geom_line(aes(y = `AR(1)`, color = "AR(1)")) +
  geom_line(aes(y = Simple, color = "Simple")) +
  geom_line(aes(y = `Broken Stick`, color = "Broken Stick")) +
  scale_color_manual(values = c("Spline" = "red", "Polynomial" = "blue",
                                "AR(1)" = "darkgreen","Simple" = "orange",
                                "Broken Stick" = "purple")) +
  labs(color = "Line") +
  theme_minimal() +
  theme(legend.position = "top") +
  guides(color = guide_legend(title = "Models"))
gg
```

Generally, the cubic spline fit looks similar to polynomial fit of degree 5.

```{r}
summary(gs)
```
Based on the results, this model fits the data significantly better than the simple straight-line model. It can be described as almost a perfect fit, given that the $R^2$ value is very close to 1.

### (g)
```{r p2g}
gsmooth <- smooth.spline(aatemp$year, aatemp$temp)
```

```{r, warning=FALSE, echo=FALSE, message=FALSE}
df.plot["Smoothing Spline"] <- gsmooth$y
gg+
  geom_line(aes(x=x, y=`Smoothing Spline`, color ="Smoothing Spline"),
            data=df.plot, show.legend=TRUE)+
  scale_color_manual(values = c("Spline" = "red", "Polynomial" = "blue",
                                "AR(1)" = "darkgreen", "Simple" = "orange",
                                "Broken Stick" = "purple",
                                "Smoothing Spline"="Magenta")) +
  labs(color = "Line") +
  theme_minimal() +
  theme(legend.position = "top") +
  guides(color = guide_legend(title = "Models"))

```
From the plot, it's hard to conclude that this model fits better than the straight-line and the spline model in (f). The fitted line appears overly wavy, suggesting that while it might be better for fitting the data, it is clearly overfitting in practice.

