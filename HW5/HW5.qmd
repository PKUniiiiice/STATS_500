---
title: "STATS 500 HW5"
author: "Minxuan Chen"
date: last-modified
format:
  html:
    toc: true
    css: styles.css
    fig-align: "center"
    fig-width: 6
    fig-height: 5
    embed-resources: true
    format-links: false
    execute:
      warning: true
      freeze: auto
  pdf:
    papersize: a4paper
    pdf-engine: pdflatex
    toc: false
    number-sections: false
    colorlinks: true
    fig-width: 6
    fig-asp: 0.8
    urlcolor: blue
    fontsize: 12pt
    geometry:
      - top=30mm
      - left=15mm
      - right=15mm
      - bottom=30mm
      - heightrounded
code-fold: show
code-overflow: scroll
code-line-numbers: true
---

```{=latex}
% Trigger ToC creation in LaTeX
\tableofcontents
\thispagestyle{empty}
\vspace{20pt}
Github repo: \url{https://github.com/PKUniiiiice/STATS_500}

\newpage

\setcounter{page}{1}
```

## Problem 1
### (a)
```{r p1a}
library(faraway)
data(teengamb)

#teengamb$sex <- as.factor(teengamb$sex)
m1 <- lm(gamble ~ ., data=teengamb)

#we use cook's distance to check for influential points
round(cooks.distance(m1), digits=4)
par(mfrow=c(1,2))
plot(m1, which=4)
abline(h=4/nrow(teengamb), col=2)
plot(m1, which=5)
par(mfrow=c(1,1))
```

From the plots, we conclude that case No.24 and No.39 are influential points.

### (b)
We use partial regression and residual plots to check the structure of the model.

Partial regression plots
```{r p1b1}
library(car)
avPlots(m1)
```

These four partial regression plots do not reveal any significant issues related to non linearity.

For outliers, each of these plots has identified two points with the largest residuals. For instance, in the `gamble-verbal` plot, cases No. 24 and No. 39 exhibit notably large-residual points and can be considered outliers.

For influential points, the plots have also identified two points with the most extreme horizontal values, signifying the large partial leverage. Some of them are merely high-leverage points (e.g., No. 34 and No. 42 in `gamble-income`), but others can be categorized as influential (e.g., No. 24 in both `gamble-sex` and `gamble-status`).

Partial residual plots
```{r p1b2, warning=FALSE, fig.align='center', fig.width=8, fig.height=6}
crPlots(m1)
```

The pink lines represent a smoother of the (component+residual) vs $x_j$. Our observations reveal that for the variables `sex` and `status`, there is no significant non linearity.

However, in `income` and `verbal`, the smoothers exhibit slight curvature. This suggests that it might be beneficial to consider adding squared terms for these variables to the model.

## Problem 2
### (1)
```{r p21}
data("longley")

#original
m_ori <- lm(Employed~., data=longley)
summary(m_ori)
#normalized
m_nor <- lm(Employed~., data=data.frame(scale(longley)))
summary(m_nor)
```

We rescale both $x$ and $y$, consequently, t-statistic (except for the intercept), F-statistic, and $R^2$ remain unchanged, but 
$$
\begin{aligned}
&\hat{\sigma} \to \hat{\sigma}/sd(y)\\
&\hat{\beta}_j \to sd(x)\cdot\hat{\beta}_j/sd(y)\\
&\hat{\beta}_0 \to (\hat{\beta}_0-\bar{y}+\Sigma\hat{\beta_j}\bar{x}_j)/sd(y)(=0)\\
\end{aligned}
$$
We can verify this
```{r p22}
temp <- scale(longley)
men <- attr(temp, "scaled:center")
sdd <- attr(temp, "scaled:scale")

#sigma^2
all.equal(summary(m_nor)$sigma,
          summary(m_ori)$sigma/sdd[7], 
          check.attributes = FALSE) 

#beta_j
all.equal(coef(m_nor)[2:7],
          coef(m_ori)[2:7]*sdd[1:6]/sdd[7],
          check.attributes = FALSE)

#beta_0
all.equal(coef(m_nor)[1],
          (coef(m_ori)[1]-men[7]+sum(coef(m_ori)[2:7]*men[1:6]))/sdd[7],
          check.attributes = FALSE)
```

Pros:   
1. We can compare coefficients directly (removing magnitude difference between predictors)
2. It helps numerical stability (numerical problems in computing $(X^TX)^{-1}$) can be avoided or mitigated).

Cons:    
1. Interpretation of coefficients is harder, since they are not in original unit.

### (2)
We calculate condition number of $X^TX$.
```{r}
kappa(crossprod(model.matrix(m_ori)))
```
It's a really large value, so there will be large collinearity in this linear model.

### (3)
```{r}
cor(longley[1:6])

library(corrplot)
corrplot(cor(longley[1:6]))
```

We observe strong correlation among `GNP.deflator`, `GNP`, `Population` and `Year`. This provides direct evidence of collinearity.

### (4)
```{r}
vif(m_ori)
```
We observe that the VIFs of `GNP.deflator`, `GNP`, `Unemployed`, `Population` and `Year` are notably high. It shows that there exists obvious collinearity.

## Problem 3
True relationship is
$$
y_i^A = \beta_0+\beta_1x_i^A
$$
There are errors in $y_i$ and $x_i$, but here we only regard errors in $y_i$ as random variable, that is
$$
y_i^O = \beta_0 + \beta_1x_i^O - \beta_1\delta_i + \epsilon_i
$$
We can regard $-\beta_1\delta_i + \epsilon_i$ as the new error term,
$$
y_i^O = \beta_0 + \beta_1x_i^O + \tilde{\epsilon}_i
$$
Despite certain assumptions of $\tilde{\epsilon}_i$ being violated compared to the classical linear regression model, we can still address this problem from a least squares perspective. In other words, we can apply the standard formula to estimate $\hat{\beta}_1$.
$$
\begin{aligned}
\hat{\beta}_1 &= \frac{\sum(x_i^O-\bar{x}^O)(y_i^O-\bar{y}^O)}{\sum(x_i^O-\bar{x}^O)^2}\\\
&=\frac{\sum (x_i^A-\bar{x}^A)(y_i^A-\bar{y}^A)+
            (x_i^A-\bar{x}^A)(\epsilon_i-\bar{\epsilon})+
            (\delta_i-\bar{\delta})(y_i^A-\bar{y}^A)+(\delta_i-\bar{\delta})(\epsilon_i-\bar{\epsilon})}{\sum (x_i^A-\bar{x}^A)^2+(\delta_i-\bar{\delta})^2 + 2(x_i^A-\bar{x}^A)(\delta_i-\bar{\delta})}\\
&= \frac{\text{numerator}}{n(\sigma_x^2+\sigma_\delta^2+2\sigma_{x\delta})} 
\end{aligned}
$$

Then, We consider the numerator. Since the errors $\epsilon_i$ are the only random variables, using $y_i^O=y_i^A+\epsilon_i$ again and $E(\epsilon_i)=0$,
$$
\begin{aligned}
\mathbb{E}(\text{numerator}) &=\sum (x_i^A-\bar{x}^A)\mathbb{E}(y_i^A-\bar{y}^A)+
            (x_i^A-\bar{x}^A)\mathbb{E}(\epsilon_i-\bar{\epsilon})+
            (\delta_i-\bar{\delta})\mathbb{E}(y_i^A-\bar{y}^A)+(\delta_i-\bar{\delta})\mathbb{E}(\epsilon_i-\bar{\epsilon}) \\
&=\sum (x_i^A-\bar{x}^A)\mathbb{E}(y_i^A-\bar{y}^A)+
            (\delta_i-\bar{\delta})\mathbb{E}(y_i^A-\bar{y}^A)\\
&= \sum (x_i^A-\bar{x}^A)\beta_1(x_i^A-\bar{x}^A)+
            (\delta_i-\bar{\delta})\beta_1(x_i^A-\bar{x}^A)\\
&= \beta_1\sum (x_i^A-\bar{x}^A)^2+
            (\delta_i-\bar{\delta})(x_i^A-\bar{x}^A)\\
            &=\beta_1\cdot n(\sigma_x^2+\sigma_{x\delta})
\end{aligned}
$$

Therefore
$$
\mathbb{E}(\hat{\beta}_1) = \frac{\mathbb{E}(\text{numerator})}{n(\sigma_x^2+\sigma_\delta^2+2\sigma_{x\delta})} =\beta_1 \frac{\sigma_x^2+\sigma_{x\delta}}{\sigma_x^2+\sigma_\delta^2+2\sigma_{x\delta}}
$$
